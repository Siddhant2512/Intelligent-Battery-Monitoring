{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Monte Carlo Dropout to PyTorch LSTM (Phase 2)\n",
        "\n",
        "This notebook adds Monte Carlo Dropout uncertainty quantification to the PyTorch LSTM model.\n",
        "\n",
        "**Why PyTorch for MC Dropout?**\n",
        "- Much simpler: Just use `model.train()` during inference\n",
        "- No need for special wrappers or functional API\n",
        "- Faster on Apple Silicon with MPS\n",
        "\n",
        "**Requirements:** \n",
        "- Run `03_train_lstm_pytorch.ipynb` first to train the base LSTM model\n",
        "- This notebook loads the saved model and adds uncertainty via MC Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import json\n",
        "import time\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path(\"/Users/siddhantaggarwal/Desktop/Battery_RUL\").resolve()\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set device\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"âœ… Using MPS (Apple Silicon GPU acceleration)\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"âœ… Using CUDA (NVIDIA GPU)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"âš ï¸ Using CPU\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"âœ… PyTorch ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "processed_dir = project_root / \"data\" / \"processed\"\n",
        "df = pd.read_parquet(processed_dir / \"rul_features_with_emd.parquet\")\n",
        "\n",
        "df_clean = df[df['RUL'].notna()].copy()\n",
        "print(f\"Dataset: {len(df_clean)} rows\")\n",
        "\n",
        "# Prepare features (same as training)\n",
        "exclude_cols = [\n",
        "    'battery_id', 'filename', 'type', 'start_time', 'test_id', 'uid',\n",
        "    'split', 'cycle_index', 'EOL_cycle', 'RUL', 'SOH', 'Capacity', \n",
        "    'Re', 'Rct', 'ambient_temperature'\n",
        "]\n",
        "\n",
        "feature_cols = [c for c in df_clean.columns if c not in exclude_cols]\n",
        "print(f\"Features: {len(feature_cols)}\")\n",
        "\n",
        "# Split by battery\n",
        "test_idx = df_clean['split'] == 'test'\n",
        "df_test = df_clean[test_idx].copy()\n",
        "print(f\"Test set: {len(df_test)} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model architecture and weights\n",
        "models_dir = project_root / \"results\" / \"models\"\n",
        "\n",
        "# Load model info\n",
        "with open(models_dir / \"lstm_pytorch_model_info.json\", 'r') as f:\n",
        "    model_info = json.load(f)\n",
        "\n",
        "print(\"Model architecture:\")\n",
        "for key, value in model_info.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Define model class (same as training)\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1=64, hidden_size2=32, num_layers=1, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size1, num_layers, \n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        \n",
        "        self.fc1 = nn.Linear(hidden_size2, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm1(x)\n",
        "        out, _ = self.lstm2(out)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out.squeeze()\n",
        "\n",
        "# Create and load model\n",
        "model = LSTMModel(\n",
        "    input_size=model_info['input_size'],\n",
        "    hidden_size1=model_info['hidden_size1'],\n",
        "    hidden_size2=model_info['hidden_size2'],\n",
        "    dropout=model_info['dropout']\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(models_dir / \"lstm_pytorch_point_model.pth\"))\n",
        "print(\"âœ… Model loaded!\")\n",
        "\n",
        "# Load scaler\n",
        "scaler = joblib.load(models_dir / \"lstm_pytorch_scaler.pkl\")\n",
        "print(\"âœ… Scaler loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sequences for test set\n",
        "sequence_length = model_info['sequence_length']\n",
        "\n",
        "def create_sequences(df_subset, feature_cols, target_col, seq_len=sequence_length):\n",
        "    \"\"\"Create sequences for LSTM.\"\"\"\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    \n",
        "    df_filled = df_subset[feature_cols + [target_col, 'battery_id', 'cycle_index']].fillna(0)\n",
        "    \n",
        "    for battery_id in df_subset['battery_id'].unique():\n",
        "        battery_data = df_filled[df_filled['battery_id'] == battery_id].sort_values('cycle_index')\n",
        "        \n",
        "        features = battery_data[feature_cols].values.astype(np.float32)\n",
        "        target = battery_data[target_col].values.astype(np.float32)\n",
        "        \n",
        "        if len(features) >= seq_len:\n",
        "            n_seqs = len(features) - seq_len\n",
        "            for i in range(n_seqs):\n",
        "                sequences.append(features[i:i+seq_len])\n",
        "                targets.append(target[i+seq_len])\n",
        "    \n",
        "    return np.array(sequences, dtype=np.float32), np.array(targets, dtype=np.float32)\n",
        "\n",
        "# Create test sequences\n",
        "X_test_seq, y_test_seq = create_sequences(df_test, feature_cols, 'RUL', sequence_length)\n",
        "\n",
        "# Normalize\n",
        "n_samples, n_timesteps, n_features = X_test_seq.shape\n",
        "X_test_reshaped = X_test_seq.reshape(-1, n_features)\n",
        "X_test_scaled = scaler.transform(X_test_reshaped)\n",
        "X_test_seq = X_test_scaled.reshape(n_samples, n_timesteps, n_features)\n",
        "\n",
        "print(f\"Test sequences: {X_test_seq.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monte Carlo Dropout Inference\n",
        "# In PyTorch, this is SUPER simple: just use model.train() during inference!\n",
        "\n",
        "n_samples_mc = 100  # Number of Monte Carlo samples\n",
        "\n",
        "print(f\"Running {n_samples_mc} Monte Carlo forward passes with dropout enabled...\")\n",
        "print(\"(This is much simpler in PyTorch - just set model.train()!)\")\n",
        "\n",
        "# Enable dropout during inference\n",
        "model.train()  # This enables dropout layers\n",
        "\n",
        "# Collect predictions\n",
        "mc_predictions = []\n",
        "\n",
        "start_time = time.time()\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    X_test_tensor = torch.FloatTensor(X_test_seq).to(device)\n",
        "    \n",
        "    for i in range(n_samples_mc):\n",
        "        pred = model(X_test_tensor)\n",
        "        mc_predictions.append(pred.cpu().numpy())\n",
        "        \n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"  Completed {i+1}/{n_samples_mc} samples...\")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"âœ… Completed in {elapsed:.2f} seconds ({elapsed/n_samples_mc*1000:.1f} ms per sample)\")\n",
        "\n",
        "# Stack predictions: shape (n_samples_mc, n_test_samples)\n",
        "mc_predictions = np.array(mc_predictions).T  # Shape: (n_test_samples, n_samples_mc)\n",
        "\n",
        "print(f\"MC predictions shape: {mc_predictions.shape} (samples, n_passes)\")\n",
        "\n",
        "# Calculate statistics\n",
        "pred_mean = np.mean(mc_predictions, axis=1)\n",
        "pred_std = np.std(mc_predictions, axis=1)\n",
        "\n",
        "# Calculate percentiles\n",
        "pred_q05 = np.percentile(mc_predictions, 5, axis=1)\n",
        "pred_q25 = np.percentile(mc_predictions, 25, axis=1)\n",
        "pred_q75 = np.percentile(mc_predictions, 75, axis=1)\n",
        "pred_q95 = np.percentile(mc_predictions, 95, axis=1)\n",
        "\n",
        "print(f\"\\nâœ… Uncertainty statistics calculated:\")\n",
        "print(f\"   Mean prediction range: [{pred_mean.min():.1f}, {pred_mean.max():.1f}]\")\n",
        "print(f\"   Uncertainty (std) range: [{pred_std.min():.2f}, {pred_std.max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate uncertainty calibration\n",
        "# Check if prediction intervals actually cover the true values\n",
        "\n",
        "# 90% prediction interval (5th to 95th percentile)\n",
        "coverage_90 = np.mean((y_test_seq >= pred_q05) & (y_test_seq <= pred_q95))\n",
        "interval_width_90 = np.mean(pred_q95 - pred_q05)\n",
        "\n",
        "# 50% prediction interval (25th to 75th percentile)\n",
        "coverage_50 = np.mean((y_test_seq >= pred_q25) & (y_test_seq <= pred_q75))\n",
        "interval_width_50 = np.mean(pred_q75 - pred_q25)\n",
        "\n",
        "print(\"Uncertainty Calibration:\")\n",
        "print(f\"  90% Prediction Interval:\")\n",
        "print(f\"    Coverage: {coverage_90*100:.1f}% (target: 90%)\")\n",
        "print(f\"    Average width: {interval_width_90:.2f} cycles\")\n",
        "print(f\"  50% Prediction Interval:\")\n",
        "print(f\"    Coverage: {coverage_50*100:.1f}% (target: 50%)\")\n",
        "print(f\"    Average width: {interval_width_50:.2f} cycles\")\n",
        "\n",
        "# Point prediction metrics (using mean of MC samples)\n",
        "def calculate_metrics(y_true, y_pred, name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-6))) * 100\n",
        "    \n",
        "    print(f\"\\n{name} Metrics (MC Dropout mean):\")\n",
        "    print(f\"  MAE:  {mae:.2f} cycles\")\n",
        "    print(f\"  RMSE: {rmse:.2f} cycles\")\n",
        "    print(f\"  RÂ²:   {r2:.3f}\")\n",
        "    print(f\"  MAPE: {mape:.2f}%\")\n",
        "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape}\n",
        "\n",
        "mc_metrics = calculate_metrics(y_test_seq, pred_mean, \"Test (MC Dropout)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions with uncertainty intervals\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Predictions vs Actual with uncertainty\n",
        "sample_idx = np.random.choice(len(y_test_seq), min(200, len(y_test_seq)), replace=False)\n",
        "sorted_idx = np.argsort(y_test_seq[sample_idx])\n",
        "\n",
        "axes[0].scatter(range(len(sorted_idx)), pred_mean[sample_idx][sorted_idx], \n",
        "                alpha=0.6, s=20, label='Mean Prediction', zorder=3)\n",
        "axes[0].fill_between(range(len(sorted_idx)), \n",
        "                     pred_q05[sample_idx][sorted_idx],\n",
        "                     pred_q95[sample_idx][sorted_idx],\n",
        "                     alpha=0.3, label='90% Prediction Interval', color='blue', zorder=1)\n",
        "axes[0].fill_between(range(len(sorted_idx)), \n",
        "                     pred_q25[sample_idx][sorted_idx],\n",
        "                     pred_q75[sample_idx][sorted_idx],\n",
        "                     alpha=0.5, label='50% Prediction Interval', color='blue', zorder=2)\n",
        "axes[0].plot(range(len(sorted_idx)), y_test_seq[sample_idx][sorted_idx], \n",
        "             'r-', linewidth=2, label='Actual', zorder=4)\n",
        "axes[0].set_xlabel('Sample Index (sorted by actual RUL)')\n",
        "axes[0].set_ylabel('RUL (cycles)')\n",
        "axes[0].set_title('LSTM Predictions with MC Dropout Uncertainty (Test Set)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Uncertainty vs Error\n",
        "abs_error = np.abs(y_test_seq - pred_mean)\n",
        "axes[1].scatter(pred_std, abs_error, alpha=0.5)\n",
        "axes[1].set_xlabel('Predicted Uncertainty (std)')\n",
        "axes[1].set_ylabel('Absolute Error')\n",
        "axes[1].set_title('Uncertainty vs Prediction Error')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Add correlation\n",
        "correlation = np.corrcoef(pred_std, abs_error)[0, 1]\n",
        "axes[1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
        "             transform=axes[1].transAxes, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results with uncertainty\n",
        "results = pd.DataFrame({\n",
        "    'actual_rul': y_test_seq,\n",
        "    'predicted_rul_mean': pred_mean,\n",
        "    'predicted_rul_std': pred_std,\n",
        "    'predicted_rul_q05': pred_q05,\n",
        "    'predicted_rul_q25': pred_q25,\n",
        "    'predicted_rul_q75': pred_q75,\n",
        "    'predicted_rul_q95': pred_q95\n",
        "})\n",
        "\n",
        "results_path = models_dir / \"lstm_pytorch_predictions_mc_dropout.csv\"\n",
        "results.to_csv(results_path, index=False)\n",
        "print(f\"âœ… Saved predictions with uncertainty: {results_path}\")\n",
        "\n",
        "# Save metrics\n",
        "metrics = pd.DataFrame({\n",
        "    'metric': ['MAE', 'RMSE', 'RÂ²', 'MAPE', 'Coverage_90%', 'Coverage_50%', 'Width_90%', 'Width_50%'],\n",
        "    'value': [\n",
        "        mc_metrics['mae'], \n",
        "        mc_metrics['rmse'], \n",
        "        mc_metrics['r2'], \n",
        "        mc_metrics['mape'],\n",
        "        coverage_90,\n",
        "        coverage_50,\n",
        "        interval_width_90,\n",
        "        interval_width_50\n",
        "    ]\n",
        "})\n",
        "\n",
        "metrics_path = models_dir / \"lstm_pytorch_metrics_mc_dropout.csv\"\n",
        "metrics.to_csv(metrics_path, index=False)\n",
        "print(f\"âœ… Saved metrics: {metrics_path}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Summary:\")\n",
        "print(f\"   Test MAE: {mc_metrics['mae']:.2f} cycles\")\n",
        "print(f\"   Test RMSE: {mc_metrics['rmse']:.2f} cycles\")\n",
        "print(f\"   Test RÂ²: {mc_metrics['r2']:.3f}\")\n",
        "print(f\"   90% Coverage: {coverage_90*100:.1f}%\")\n",
        "print(f\"   50% Coverage: {coverage_50*100:.1f}%\")\n",
        "print(f\"\\nâœ… Phase 2 Complete: Monte Carlo Dropout uncertainty quantification added!\")\n",
        "print(f\"   Uncertainty intervals saved and ready for dashboard!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
