{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Random Forest Model (Point Prediction)\n",
        "\n",
        "This notebook trains a Random Forest model for RUL prediction.\n",
        "\n",
        "**Phase 1**: Point prediction only - no uncertainty quantification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path(\"/Users/siddhantaggarwal/Desktop/Battery_RUL\").resolve()\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset with EMD features\n",
        "processed_dir = project_root / \"data\" / \"processed\"\n",
        "df = pd.read_parquet(processed_dir / \"rul_features_with_emd.parquet\")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nSplits distribution:\")\n",
        "print(df['split'].value_counts().sort_index())\n",
        "\n",
        "# Filter out rows with NaN RUL (batteries that didn't reach EOL)\n",
        "df_clean = df[df['RUL'].notna()].copy()\n",
        "print(f\"\\nAfter removing NaN RUL: {len(df_clean)} rows\")\n",
        "print(f\"RUL range: [{df_clean['RUL'].min():.1f}, {df_clean['RUL'].max():.1f}] cycles\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "exclude_cols = [\n",
        "    'battery_id', 'filename', 'type', 'start_time', 'test_id', 'uid',\n",
        "    'split', 'cycle_index', 'EOL_cycle', 'RUL', 'SOH', 'Capacity', \n",
        "    'Re', 'Rct', 'ambient_temperature'\n",
        "]\n",
        "\n",
        "feature_cols = [c for c in df_clean.columns if c not in exclude_cols]\n",
        "print(f\"Total features: {len(feature_cols)}\")\n",
        "print(f\"\\nFeature categories:\")\n",
        "emd_features = [c for c in feature_cols if '_imf' in c.lower()]\n",
        "stat_features = [c for c in feature_cols if c not in emd_features]\n",
        "print(f\"  - Statistical features: {len(stat_features)}\")\n",
        "print(f\"  - EMD features: {len(emd_features)}\")\n",
        "\n",
        "# Create feature matrix and target\n",
        "X = df_clean[feature_cols].fillna(0)\n",
        "y = df_clean['RUL'].values\n",
        "\n",
        "# Split by battery (already done in dataset)\n",
        "train_idx = df_clean['split'] == 'train'\n",
        "val_idx = df_clean['split'] == 'val'\n",
        "test_idx = df_clean['split'] == 'test'\n",
        "\n",
        "X_train, y_train = X[train_idx], y[train_idx]\n",
        "X_val, y_val = X[val_idx], y[val_idx]\n",
        "X_test, y_test = X[test_idx], y[test_idx]\n",
        "\n",
        "print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "print(f\"Train RUL stats: mean={y_train.mean():.2f}, std={y_train.std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest for point prediction\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"‚úÖ Model trained!\")\n",
        "\n",
        "# Point predictions\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Handle validation set (may be empty after filtering)\n",
        "if len(X_val) > 0:\n",
        "    y_val_pred = rf_model.predict(X_val)\n",
        "    print(f\"‚úÖ Validation set has {len(X_val)} samples\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Validation set is empty after filtering NaN RUL values\")\n",
        "    print(\"   This happens when validation batteries didn't reach EOL (SOH <= 0.8)\")\n",
        "    print(\"   This is acceptable - we'll use train/test splits only\")\n",
        "    y_val_pred = np.array([])\n",
        "\n",
        "# Metrics calculation function\n",
        "def calculate_metrics(y_true, y_pred, name):\n",
        "    if len(y_pred) == 0:\n",
        "        print(f\"\\n{name} Metrics: No data available (skipped)\")\n",
        "        return {'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'mape': np.nan}\n",
        "    \n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-6))) * 100\n",
        "    \n",
        "    print(f\"\\n{name} Metrics:\")\n",
        "    print(f\"  MAE:  {mae:.2f} cycles\")\n",
        "    print(f\"  RMSE: {rmse:.2f} cycles\")\n",
        "    print(f\"  R¬≤:   {r2:.3f}\")\n",
        "    print(f\"  MAPE: {mape:.2f}%\")\n",
        "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape}\n",
        "\n",
        "# Calculate metrics\n",
        "train_metrics = calculate_metrics(y_train, y_train_pred, \"Train\")\n",
        "if len(X_val) > 0:\n",
        "    val_metrics = calculate_metrics(y_val, y_val_pred, \"Validation\")\n",
        "else:\n",
        "    val_metrics = {'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'mape': np.nan}\n",
        "    print(\"\\n‚ö†Ô∏è  Skipping validation metrics (empty set)\")\n",
        "test_metrics = calculate_metrics(y_test, y_test_pred, \"Test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 15 Most Important Features:\")\n",
        "print(feature_importance.head(15).to_string(index=False))\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_n = 20\n",
        "top_features = feature_importance.head(top_n)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title(f'Top {top_n} Feature Importances (Random Forest)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Predictions vs Actual\n",
        "sample_idx = np.random.choice(len(y_test), min(200, len(y_test)), replace=False)\n",
        "sorted_idx = np.argsort(y_test[sample_idx])\n",
        "\n",
        "axes[0].scatter(y_test[sample_idx][sorted_idx], y_test_pred[sample_idx][sorted_idx], \n",
        "                alpha=0.5, s=20, label='Predictions')\n",
        "axes[0].plot([y_test[sample_idx].min(), y_test[sample_idx].max()], \n",
        "             [y_test[sample_idx].min(), y_test[sample_idx].max()], \n",
        "             'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Actual RUL')\n",
        "axes[0].set_ylabel('Predicted RUL')\n",
        "axes[0].set_title('Predictions vs Actual (Test Set)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Residuals plot\n",
        "residuals = y_test - y_test_pred\n",
        "axes[1].scatter(y_test_pred, residuals, alpha=0.5)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--')\n",
        "axes[1].set_xlabel('Predicted RUL')\n",
        "axes[1].set_ylabel('Residual (Actual - Predicted)')\n",
        "axes[1].set_title('Residuals Plot')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and results\n",
        "models_dir = project_root / \"results\" / \"models\"\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "model_path = models_dir / \"random_forest_rul_point_model.pkl\"\n",
        "joblib.dump(rf_model, model_path)\n",
        "print(f\"‚úÖ Saved model: {model_path}\")\n",
        "\n",
        "# Save predictions\n",
        "results = pd.DataFrame({\n",
        "    'battery_id': df_clean.loc[test_idx, 'battery_id'].values,\n",
        "    'cycle_index': df_clean.loc[test_idx, 'cycle_index'].values,\n",
        "    'actual_rul': y_test,\n",
        "    'predicted_rul': y_test_pred,\n",
        "})\n",
        "\n",
        "results_path = models_dir / \"rf_predictions_point.csv\"\n",
        "results.to_csv(results_path, index=False)\n",
        "print(f\"‚úÖ Saved predictions: {results_path}\")\n",
        "\n",
        "# Save metrics\n",
        "metrics = pd.DataFrame({\n",
        "    'metric': ['MAE', 'RMSE', 'R¬≤', 'MAPE'],\n",
        "    'train': [train_metrics['mae'], train_metrics['rmse'], train_metrics['r2'], train_metrics['mape']],\n",
        "    'val': [val_metrics['mae'], val_metrics['rmse'], val_metrics['r2'], val_metrics['mape']],\n",
        "    'test': [test_metrics['mae'], test_metrics['rmse'], test_metrics['r2'], test_metrics['mape']]\n",
        "})\n",
        "\n",
        "metrics_path = models_dir / \"rf_metrics_point.csv\"\n",
        "metrics.to_csv(metrics_path, index=False)\n",
        "print(f\"‚úÖ Saved metrics: {metrics_path}\")\n",
        "\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   Test MAE: {test_metrics['mae']:.2f} cycles\")\n",
        "print(f\"   Test RMSE: {test_metrics['rmse']:.2f} cycles\")\n",
        "print(f\"   Test R¬≤: {test_metrics['r2']:.3f}\")\n",
        "print(f\"\\n‚úÖ Phase 1 Complete: Random Forest point prediction model saved!\")\n",
        "print(f\"   Next: Train LSTM and Transformer models, then compare all 3.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
