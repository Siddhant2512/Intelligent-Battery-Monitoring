{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Models with Reduced Features (PCA)\n",
    "\n",
    "This notebook retrains LSTM and other models using the PCA-reduced feature set (57 components instead of 175).\n",
    "\n",
    "**Feature Selection Results:**\n",
    "- Original features: 175 (16 statistical + 159 EMD)\n",
    "- After redundancy removal: 145 features\n",
    "- PCA (95% variance): 57 components\n",
    "- Best strategy: PCA (95% variance) - Test MAE: 20.110 cycles, RÂ²: 0.157\n",
    "\n",
    "**Goals:**\n",
    "1. Load PCA transformers and feature selection info\n",
    "2. Apply PCA transformation to features\n",
    "3. Retrain LSTM with 57-component features\n",
    "4. Retrain Random Forest with reduced features\n",
    "5. Compare performance: original vs reduced features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using MPS (Apple Silicon GPU acceleration)\n",
      "PyTorch version: 2.9.0\n",
      "Device: mps\n",
      "âœ… Imports complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch for LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Bayesian Optimization for LSTM hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"/Users/siddhantaggarwal/Desktop/Battery_RUL\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"âœ… Using MPS (Apple Silicon GPU acceleration)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"âœ… Using CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"âœ… Imports complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Feature Selection Info and Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Feature Selection Info:\n",
      "  Method: pca\n",
      "  Strategy: PCA (95% variance)\n",
      "  PCA Components: 57\n",
      "  Features after redundancy removal: 145\n",
      "  âœ… PCA transformers loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load feature selection info\n",
    "models_dir = project_root / \"results\" / \"models\"\n",
    "with open(models_dir / \"feature_selection_info.json\", 'r') as f:\n",
    "    feature_selection_info = json.load(f)\n",
    "\n",
    "print(\"ðŸ“Š Feature Selection Info:\")\n",
    "print(f\"  Method: {feature_selection_info['method']}\")\n",
    "print(f\"  Strategy: {feature_selection_info['strategy']}\")\n",
    "\n",
    "if feature_selection_info['method'] == 'pca':\n",
    "    n_components = feature_selection_info['n_components']\n",
    "    features_after_redundancy = feature_selection_info['features_after_redundancy_removal']\n",
    "    \n",
    "    # Load PCA and scaler\n",
    "    pca = joblib.load(models_dir / \"feature_selection_pca.pkl\")\n",
    "    scaler_pca = joblib.load(models_dir / \"feature_selection_scaler.pkl\")\n",
    "    \n",
    "    print(f\"  PCA Components: {n_components}\")\n",
    "    print(f\"  Features after redundancy removal: {len(features_after_redundancy)}\")\n",
    "    print(f\"  âœ… PCA transformers loaded!\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ Feature selection method '{feature_selection_info['method']}' not yet supported\")\n",
    "    print(f\"  Please use PCA method for now\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data and Apply PCA Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2750, 190)\n",
      "After removing NaN RUL: 1408 rows\n",
      "\n",
      "Features after redundancy removal: 145\n",
      "\n",
      "âœ… PCA transformation applied!\n",
      "  Original features: 145\n",
      "  PCA components: 57\n",
      "  Train shape: (1158, 57)\n",
      "  Test shape: (250, 57)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "processed_dir = project_root / \"data\" / \"processed\"\n",
    "df = pd.read_parquet(processed_dir / \"rul_features_with_emd.parquet\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Filter out rows with NaN RUL\n",
    "df_clean = df[df['RUL'].notna()].copy()\n",
    "print(f\"After removing NaN RUL: {len(df_clean)} rows\")\n",
    "\n",
    "# Prepare features (use features after redundancy removal)\n",
    "exclude_cols = [\n",
    "    'battery_id', 'filename', 'type', 'start_time', 'test_id', 'uid',\n",
    "    'split', 'cycle_index', 'EOL_cycle', 'RUL', 'SOH', 'Capacity', \n",
    "    'Re', 'Rct', 'ambient_temperature'\n",
    "]\n",
    "\n",
    "# Use features after redundancy removal\n",
    "feature_cols_reduced = features_after_redundancy\n",
    "print(f\"\\nFeatures after redundancy removal: {len(feature_cols_reduced)}\")\n",
    "\n",
    "# Split by battery\n",
    "train_idx = df_clean['split'] == 'train'\n",
    "test_idx = df_clean['split'] == 'test'\n",
    "\n",
    "df_train = df_clean[train_idx].copy()\n",
    "df_test = df_clean[test_idx].copy()\n",
    "\n",
    "# Extract features and apply PCA transformation\n",
    "X_train_raw = df_train[feature_cols_reduced].fillna(0).values\n",
    "X_test_raw = df_test[feature_cols_reduced].fillna(0).values\n",
    "\n",
    "# Standardize and apply PCA\n",
    "X_train_scaled = scaler_pca.transform(X_train_raw)\n",
    "X_test_scaled = scaler_pca.transform(X_test_raw)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_scaled)[:, :n_components]\n",
    "X_test_pca = pca.transform(X_test_scaled)[:, :n_components]\n",
    "\n",
    "print(f\"\\nâœ… PCA transformation applied!\")\n",
    "print(f\"  Original features: {len(feature_cols_reduced)}\")\n",
    "print(f\"  PCA components: {n_components}\")\n",
    "print(f\"  Train shape: {X_train_pca.shape}\")\n",
    "print(f\"  Test shape: {X_test_pca.shape}\")\n",
    "\n",
    "# Get targets\n",
    "y_train = df_train['RUL'].values\n",
    "y_test = df_test['RUL'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Retrain Random Forest with Reduced Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with PCA-reduced features...\n",
      "\n",
      "âœ… Random Forest trained!\n",
      "\n",
      "Train Metrics:\n",
      "  MAE: 7.61 cycles\n",
      "  RMSE: 10.00 cycles\n",
      "  RÂ²: 0.949\n",
      "\n",
      "Test Metrics:\n",
      "  MAE: 20.08 cycles\n",
      "  RMSE: 24.89 cycles\n",
      "  RÂ²: 0.160\n",
      "\n",
      "âœ… Saved model to /Users/siddhantaggarwal/Desktop/Battery_RUL/results/models/random_forest_rul_point_model_reduced.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest with PCA features using GridSearchCV for hyperparameter optimization\n",
    "print(\"Training Random Forest with PCA-reduced features...\")\n",
    "print(\"Performing GridSearchCV to find best hyperparameters...\")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 15, 20, 25, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Base Random Forest model\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Perform GridSearchCV with cross-validation\n",
    "print(\"Starting GridSearchCV (this may take a few minutes)...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',  # Use MAE as scoring metric\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Get best model\n",
    "rf_reduced = grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\nâœ… GridSearchCV completed!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score (MAE): {-grid_search.best_score_:.2f} cycles\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_reduced.predict(X_train_pca)\n",
    "y_test_pred_rf = rf_reduced.predict(X_test_pca)\n",
    "\n",
    "# Metrics\n",
    "train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
    "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
    "\n",
    "test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(\"\\nâœ… Random Forest trained with optimized hyperparameters!\")\n",
    "print(f\"\\nTrain Metrics:\")\n",
    "print(f\"  MAE: {train_mae_rf:.2f} cycles\")\n",
    "print(f\"  RMSE: {train_rmse_rf:.2f} cycles\")\n",
    "print(f\"  RÂ²: {train_r2_rf:.3f}\")\n",
    "\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  MAE: {test_mae_rf:.2f} cycles\")\n",
    "print(f\"  RMSE: {test_rmse_rf:.2f} cycles\")\n",
    "print(f\"  RÂ²: {test_r2_rf:.3f}\")\n",
    "\n",
    "# Save model and best parameters\n",
    "joblib.dump(rf_reduced, models_dir / \"random_forest_rul_point_model_reduced.pkl\")\n",
    "joblib.dump(grid_search.best_params_, models_dir / \"random_forest_reduced_best_params.pkl\")\n",
    "print(f\"\\nâœ… Saved model and best parameters to {models_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrain LSTM with Reduced Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sequences with length 20...\n",
      "Train sequences: (918, 20, 57)\n",
      "Test sequences: (190, 20, 57)\n",
      "Feature shape per timestep: 57 (reduced from 175)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences for LSTM with PCA features\n",
    "sequence_length = 20\n",
    "\n",
    "def create_sequences_pca(df_subset, pca_features, target_col, seq_len=sequence_length):\n",
    "    \"\"\"Create sequences for LSTM training with PCA features.\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group by battery_id\n",
    "    for battery_id in df_subset['battery_id'].unique():\n",
    "        battery_mask = df_subset['battery_id'] == battery_id\n",
    "        battery_indices = df_subset[battery_mask].sort_values('cycle_index').index\n",
    "        \n",
    "        # Get PCA features and targets for this battery\n",
    "        battery_pca = pca_features[battery_mask][np.argsort(df_subset[battery_mask]['cycle_index'])]\n",
    "        battery_target = df_subset.loc[battery_indices, target_col].values.astype(np.float32)\n",
    "        \n",
    "        if len(battery_pca) >= seq_len:\n",
    "            n_seqs = len(battery_pca) - seq_len\n",
    "            for i in range(n_seqs):\n",
    "                sequences.append(battery_pca[i:i+seq_len])\n",
    "                targets.append(battery_target[i+seq_len])\n",
    "    \n",
    "    return np.array(sequences, dtype=np.float32), np.array(targets, dtype=np.float32)\n",
    "\n",
    "print(f\"Creating sequences with length {sequence_length}...\")\n",
    "X_train_seq, y_train_seq = create_sequences_pca(df_train, X_train_pca, 'RUL', sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences_pca(df_test, X_test_pca, 'RUL', sequence_length)\n",
    "\n",
    "print(f\"Train sequences: {X_train_seq.shape}\")\n",
    "print(f\"Test sequences: {X_test_seq.shape}\")\n",
    "print(f\"Feature shape per timestep: {X_train_seq.shape[2]} (reduced from 175)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model created with input size: 57\n",
      "   Total parameters: 44,577\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM model (same architecture as original, but with reduced input size)\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM model architecture.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size1=64, hidden_size2=32, num_layers=1, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size1, num_layers, \n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc1 = nn.Linear(hidden_size2, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "# Dataset class\n",
    "class BatteryDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "# Create model with reduced input size\n",
    "input_size = n_components  # 57 instead of 175\n",
    "model = LSTMModel(input_size=input_size, hidden_size1=64, hidden_size2=32, dropout=0.2).to(device)\n",
    "\n",
    "print(f\"âœ… Model created with input size: {input_size}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features normalized for LSTM\n",
      "Training on 734 samples, validating on 184 samples\n"
     ]
    }
   ],
   "source": [
    "# Normalize features for LSTM (StandardScaler on sequences)\n",
    "# Reshape sequences to (n_samples * seq_len, n_features) for scaling\n",
    "n_train, seq_len, n_feat = X_train_seq.shape\n",
    "n_test = X_test_seq.shape[0]\n",
    "\n",
    "X_train_flat = X_train_seq.reshape(-1, n_feat)\n",
    "X_test_flat = X_test_seq.reshape(-1, n_feat)\n",
    "\n",
    "scaler_lstm = StandardScaler()\n",
    "X_train_scaled_flat = scaler_lstm.fit_transform(X_train_flat)\n",
    "X_test_scaled_flat = scaler_lstm.transform(X_test_flat)\n",
    "\n",
    "# Reshape back\n",
    "X_train_seq_scaled = X_train_scaled_flat.reshape(n_train, seq_len, n_feat)\n",
    "X_test_seq_scaled = X_test_scaled_flat.reshape(n_test, seq_len, n_feat)\n",
    "\n",
    "print(\"âœ… Features normalized for LSTM\")\n",
    "\n",
    "# Create datasets and loaders\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train_seq_scaled, y_train_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = BatteryDataset(X_train_final, y_train_final)\n",
    "val_dataset = BatteryDataset(X_val_final, y_val_final)\n",
    "test_dataset = BatteryDataset(X_test_seq_scaled, y_test_seq)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 10/50: Train Loss: 628.8212, Val Loss: 622.2708\n",
      "Epoch 20/50: Train Loss: 163.4858, Val Loss: 165.3811\n",
      "Epoch 30/50: Train Loss: 105.1973, Val Loss: 68.6324\n",
      "Epoch 40/50: Train Loss: 76.0852, Val Loss: 28.6537\n",
      "Epoch 50/50: Train Loss: 70.3124, Val Loss: 17.4374\n",
      "âœ… Loaded best model weights\n",
      "Training completed in 6.6 seconds\n"
     ]
    }
   ],
   "source": [
    "# Bayesian Optimization with Optuna for LSTM hyperparameter tuning\n",
    "print(\"Starting Bayesian Optimization with Optuna to find best hyperparameters...\")\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna optimization.\"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size1 = trial.suggest_int('hidden_size1', 32, 128, step=16)\n",
    "    hidden_size2 = trial.suggest_int('hidden_size2', 16, 64, step=8)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 30, 80, step=10)\n",
    "    \n",
    "    # Create model with suggested hyperparameters\n",
    "    model_trial = LSTMModel(\n",
    "        input_size=input_size,\n",
    "        hidden_size1=hidden_size1,\n",
    "        hidden_size2=hidden_size2,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_trial.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create data loaders with suggested batch size\n",
    "    train_loader_trial = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_trial = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model_trial.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader_trial:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_trial(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model_trial.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader_trial:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs = model_trial(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader_trial)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name='lstm_hyperparameter_optimization',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "print(\"Running Optuna optimization (this may take a while)...\")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ… Optuna optimization completed!\")\n",
    "print(f\"Best trial:\")\n",
    "print(f\"  Value (validation loss): {study.best_value:.4f}\")\n",
    "print(f\"  Params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(f\"\\nTraining final model with best hyperparameters...\")\n",
    "\n",
    "# Create final model with best hyperparameters\n",
    "model = LSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size1=best_params['hidden_size1'],\n",
    "    hidden_size2=best_params['hidden_size2'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Training setup with best hyperparameters\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Create data loaders with best batch size\n",
    "batch_size = best_params['batch_size']\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = best_params['num_epochs']\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting final training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), models_dir / \"lstm_pytorch_reduced_best_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(models_dir / \"lstm_pytorch_reduced_best_model.pth\"))\n",
    "print(\"âœ… Loaded best model weights\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Training completed in {elapsed_time:.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… LSTM Evaluation Complete!\n",
      "\n",
      "Train Metrics:\n",
      "  MAE: 2.31 cycles\n",
      "  RMSE: 3.70 cycles\n",
      "  RÂ²: 0.992\n",
      "\n",
      "Test Metrics:\n",
      "  MAE: 15.54 cycles\n",
      "  RMSE: 20.51 cycles\n",
      "  RÂ²: 0.145\n",
      "\n",
      "âœ… Saved model and info to /Users/siddhantaggarwal/Desktop/Battery_RUL/results/models\n"
     ]
    }
   ],
   "source": [
    "# Evaluate LSTM model\n",
    "model.eval()\n",
    "train_preds = []\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0):\n",
    "        batch_X = batch_X.to(device)\n",
    "        preds = model(batch_X)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    for batch_X, _ in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        preds = model(batch_X)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "train_preds = np.array(train_preds)\n",
    "test_preds = np.array(test_preds)\n",
    "\n",
    "# Metrics\n",
    "train_mae_lstm = mean_absolute_error(y_train_final, train_preds)\n",
    "train_rmse_lstm = np.sqrt(mean_squared_error(y_train_final, train_preds))\n",
    "train_r2_lstm = r2_score(y_train_final, train_preds)\n",
    "\n",
    "test_mae_lstm = mean_absolute_error(y_test_seq, test_preds)\n",
    "test_rmse_lstm = np.sqrt(mean_squared_error(y_test_seq, test_preds))\n",
    "test_r2_lstm = r2_score(y_test_seq, test_preds)\n",
    "\n",
    "print(\"\\nâœ… LSTM Evaluation Complete!\")\n",
    "print(f\"\\nTrain Metrics:\")\n",
    "print(f\"  MAE: {train_mae_lstm:.2f} cycles\")\n",
    "print(f\"  RMSE: {train_rmse_lstm:.2f} cycles\")\n",
    "print(f\"  RÂ²: {train_r2_lstm:.3f}\")\n",
    "\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  MAE: {test_mae_lstm:.2f} cycles\")\n",
    "print(f\"  RMSE: {test_rmse_lstm:.2f} cycles\")\n",
    "print(f\"  RÂ²: {test_r2_lstm:.3f}\")\n",
    "\n",
    "# Save model and scaler\n",
    "torch.save(model.state_dict(), models_dir / \"lstm_pytorch_reduced_point_model.pth\")\n",
    "joblib.dump(scaler_lstm, models_dir / \"lstm_pytorch_reduced_scaler.pkl\")\n",
    "\n",
    "# Save model info with best hyperparameters\n",
    "model_info = {\n",
    "    'input_size': n_components,\n",
    "    'hidden_size1': best_params['hidden_size1'],\n",
    "    'hidden_size2': best_params['hidden_size2'],\n",
    "    'num_layers': 1,\n",
    "    'dropout': best_params['dropout'],\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'sequence_length': sequence_length,\n",
    "    'feature_selection_method': 'pca',\n",
    "    'n_components': n_components,\n",
    "    'optimization_method': 'optuna_bayesian',\n",
    "    'best_validation_loss': study.best_value\n",
    "}\n",
    "\n",
    "with open(models_dir / \"lstm_pytorch_reduced_model_info.json\", 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Saved model and info to {models_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare Performance (Original vs Reduced Features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Performance Comparison: Original vs Reduced Features\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ”µ LSTM Model:\n",
      "Metric          Original (175)       Reduced (57)         Change         \n",
      "----------------------------------------------------------------------\n",
      "Test MAE        16.74                15.54                -1.20\n",
      "Test RMSE       20.35                20.51                +0.16\n",
      "Test RÂ²         0.159                0.145                -0.014\n",
      "\n",
      "ðŸŸ¢ Random Forest Model:\n",
      "Metric          Original (175)       Reduced (57)         Change         \n",
      "----------------------------------------------------------------------\n",
      "Test MAE        21.28                20.08                -1.20\n",
      "Test RMSE       27.89                24.89                -3.00\n",
      "Test RÂ²         -0.055               0.160                +0.215\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… LSTM: IMPROVED (MAE: 15.54 vs 16.74)\n",
      "âœ… Random Forest: IMPROVED (MAE: 20.08 vs 21.28)\n",
      "\n",
      "ðŸ“‰ Overfitting Analysis (LSTM):\n",
      "  Original: Train RÂ² - Test RÂ² = 0.597\n",
      "  Reduced: Train RÂ² - Test RÂ² = 0.846\n",
      "  Overfitting INCREASED\n"
     ]
    }
   ],
   "source": [
    "# Load original model metrics for comparison\n",
    "try:\n",
    "    # Try to load original LSTM metrics\n",
    "    original_lstm_info_path = models_dir / \"lstm_pytorch_model_info.json\"\n",
    "    if original_lstm_info_path.exists():\n",
    "        # Original LSTM metrics (from previous training)\n",
    "        original_lstm_test_mae = 16.74\n",
    "        original_lstm_test_rmse = 20.35\n",
    "        original_lstm_test_r2 = 0.159\n",
    "        \n",
    "        # Original RF metrics\n",
    "        original_rf_test_mae = 21.28\n",
    "        original_rf_test_rmse = 27.89\n",
    "        original_rf_test_r2 = -0.055\n",
    "        \n",
    "        print(\"ðŸ“Š Performance Comparison: Original vs Reduced Features\\n\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # LSTM Comparison\n",
    "        print(\"\\nðŸ”µ LSTM Model:\")\n",
    "        print(f\"{'Metric':<15} {'Original (175)':<20} {'Reduced (57)':<20} {'Change':<15}\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"{'Test MAE':<15} {original_lstm_test_mae:<20.2f} {test_mae_lstm:<20.2f} {test_mae_lstm - original_lstm_test_mae:+.2f}\")\n",
    "        print(f\"{'Test RMSE':<15} {original_lstm_test_rmse:<20.2f} {test_rmse_lstm:<20.2f} {test_rmse_lstm - original_lstm_test_rmse:+.2f}\")\n",
    "        print(f\"{'Test RÂ²':<15} {original_lstm_test_r2:<20.3f} {test_r2_lstm:<20.3f} {test_r2_lstm - original_lstm_test_r2:+.3f}\")\n",
    "        \n",
    "        # RF Comparison\n",
    "        print(\"\\nðŸŸ¢ Random Forest Model:\")\n",
    "        print(f\"{'Metric':<15} {'Original (175)':<20} {'Reduced (57)':<20} {'Change':<15}\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"{'Test MAE':<15} {original_rf_test_mae:<20.2f} {test_mae_rf:<20.2f} {test_mae_rf - original_rf_test_mae:+.2f}\")\n",
    "        print(f\"{'Test RMSE':<15} {original_rf_test_rmse:<20.2f} {test_rmse_rf:<20.2f} {test_rmse_rf - original_rf_test_rmse:+.2f}\")\n",
    "        print(f\"{'Test RÂ²':<15} {original_rf_test_r2:<20.3f} {test_r2_rf:<20.3f} {test_r2_rf - original_rf_test_r2:+.3f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # Determine if reduced features improved performance\n",
    "        lstm_improved = test_mae_lstm < original_lstm_test_mae\n",
    "        rf_improved = test_mae_rf < original_rf_test_mae\n",
    "        \n",
    "        print(f\"\\nâœ… LSTM: {'IMPROVED' if lstm_improved else 'WORSE'} (MAE: {test_mae_lstm:.2f} vs {original_lstm_test_mae:.2f})\")\n",
    "        print(f\"âœ… Random Forest: {'IMPROVED' if rf_improved else 'WORSE'} (MAE: {test_mae_rf:.2f} vs {original_rf_test_mae:.2f})\")\n",
    "        \n",
    "        # Check overfitting reduction\n",
    "        # For original: Train RÂ² = 0.756, Test RÂ² = 0.159\n",
    "        # For reduced: Use train RÂ² from training split, Test RÂ² from test set\n",
    "        lstm_overfit_original = 0.756 - 0.159  # Train RÂ² - Test RÂ² (from original)\n",
    "        lstm_overfit_reduced = train_r2_lstm - test_r2_lstm\n",
    "        \n",
    "        print(f\"\\nðŸ“‰ Overfitting Analysis (LSTM):\")\n",
    "        print(f\"  Original: Train RÂ² - Test RÂ² = {lstm_overfit_original:.3f}\")\n",
    "        print(f\"  Reduced: Train RÂ² - Test RÂ² = {lstm_overfit_reduced:.3f}\")\n",
    "        print(f\"  Overfitting {'REDUCED' if lstm_overfit_reduced < lstm_overfit_original else 'INCREASED'}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not load original metrics: {e}\")\n",
    "    print(\"Showing only reduced feature results above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (3), usually from a call to set_ticks, does not match the number of labels (2).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m axes[i].set_title(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Comparison\u001b[39m\u001b[33m'\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m axes[i].set_xticks(x)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_xticklabels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLSTM\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRandom Forest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m axes[i].legend()\n\u001b[32m     24\u001b[39m axes[i].grid(\u001b[38;5;28;01mTrue\u001b[39;00m, alpha=\u001b[32m0.3\u001b[39m, axis=\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/matplotlib/axes/_base.py:74\u001b[39m, in \u001b[36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/matplotlib/axis.py:2106\u001b[39m, in \u001b[36mAxis.set_ticklabels\u001b[39m\u001b[34m(self, labels, minor, fontdict, **kwargs)\u001b[39m\n\u001b[32m   2102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(locator, mticker.FixedLocator):\n\u001b[32m   2103\u001b[39m     \u001b[38;5;66;03m# Passing [] as a list of labels is often used as a way to\u001b[39;00m\n\u001b[32m   2104\u001b[39m     \u001b[38;5;66;03m# remove all tick labels, so only error for > 0 labels\u001b[39;00m\n\u001b[32m   2105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(locator.locs) != \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) != \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2106\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe number of FixedLocator locations\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2108\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(locator.locs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), usually from a call to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2109\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m set_ticks, does not match\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2110\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m the number of labels (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2111\u001b[39m     tickd = {loc: lab \u001b[38;5;28;01mfor\u001b[39;00m loc, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(locator.locs, labels)}\n\u001b[32m   2112\u001b[39m     func = functools.partial(\u001b[38;5;28mself\u001b[39m._format_with_dict, tickd)\n",
      "\u001b[31mValueError\u001b[39m: The number of FixedLocator locations (3), usually from a call to set_ticks, does not match the number of labels (2)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'RÂ²']\n",
    "original_lstm = [original_lstm_test_mae, original_lstm_test_rmse, original_lstm_test_r2]\n",
    "reduced_lstm = [test_mae_lstm, test_rmse_lstm, test_r2_lstm]\n",
    "original_rf = [original_rf_test_mae, original_rf_test_rmse, original_rf_test_r2]\n",
    "reduced_rf = [test_mae_rf, test_rmse_rf, test_r2_rf]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "for i, (metric, orig_lstm, red_lstm, orig_rf, red_rf) in enumerate(zip(metrics, original_lstm, reduced_lstm, original_rf, reduced_rf)):\n",
    "    axes[i].bar(x[0] - width/2, orig_lstm, width, label='Original (175)', color='steelblue', alpha=0.7)\n",
    "    axes[i].bar(x[0] + width/2, red_lstm, width, label='Reduced (57)', color='coral', alpha=0.7)\n",
    "    axes[i].bar(x[1] - width/2, orig_rf, width, color='steelblue', alpha=0.7)\n",
    "    axes[i].bar(x[1] + width/2, red_rf, width, color='coral', alpha=0.7)\n",
    "    \n",
    "    axes[i].set_ylabel(metric, fontweight='bold')\n",
    "    axes[i].set_title(f'{metric} Comparison', fontweight='bold')\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels(['LSTM', 'Random Forest'])\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Comparison visualization complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battery_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
