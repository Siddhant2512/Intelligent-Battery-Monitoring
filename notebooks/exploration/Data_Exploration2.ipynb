{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original metadata head:\n",
      "        type                                         start_time  \\\n",
      "0  discharge  [2010.       7.      21.      15.       0.    ...   \n",
      "1  impedance  [2010.       7.      21.      16.      53.    ...   \n",
      "2     charge  [2010.       7.      21.      17.      25.    ...   \n",
      "3  impedance                    [2010    7   21   20   31    5]   \n",
      "4  discharge  [2.0100e+03 7.0000e+00 2.1000e+01 2.1000e+01 2...   \n",
      "\n",
      "   ambient_temperature battery_id  test_id  uid   filename  \\\n",
      "0                    4      B0047        0    1  00001.csv   \n",
      "1                   24      B0047        1    2  00002.csv   \n",
      "2                    4      B0047        2    3  00003.csv   \n",
      "3                   24      B0047        3    4  00004.csv   \n",
      "4                    4      B0047        4    5  00005.csv   \n",
      "\n",
      "             Capacity                   Re                  Rct  \n",
      "0  1.6743047446975208                  NaN                  NaN  \n",
      "1                 NaN  0.05605783343888099  0.20097016584458333  \n",
      "2                 NaN                  NaN                  NaN  \n",
      "3                 NaN  0.05319185850921101  0.16473399914864734  \n",
      "4  1.5243662105099023                  NaN                  NaN  \n",
      "\n",
      "Metadata info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7565 entries, 0 to 7564\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   type                 7565 non-null   object\n",
      " 1   start_time           7565 non-null   object\n",
      " 2   ambient_temperature  7565 non-null   int64 \n",
      " 3   battery_id           7565 non-null   object\n",
      " 4   test_id              7565 non-null   int64 \n",
      " 5   uid                  7565 non-null   int64 \n",
      " 6   filename             7565 non-null   object\n",
      " 7   Capacity             2794 non-null   object\n",
      " 8   Re                   1956 non-null   object\n",
      " 9   Rct                  1956 non-null   object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 591.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "metadata_path = '/Users/siddhantaggarwal/Desktop/Battery_RUL/cleaned_dataset/metadata.csv' # Make sure this path is correct\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "print(\"Original metadata head:\")\n",
    "print(metadata.head())\n",
    "print(\"\\nMetadata info:\")\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discharge metadata head:\n",
      "         type                                         start_time  \\\n",
      "0   discharge  [2010.       7.      21.      15.       0.    ...   \n",
      "4   discharge  [2.0100e+03 7.0000e+00 2.1000e+01 2.1000e+01 2...   \n",
      "6   discharge  [2.010e+03 7.000e+00 2.200e+01 1.000e+00 4.000...   \n",
      "8   discharge  [2010.       7.      22.       6.      16.    ...   \n",
      "10  discharge  [2010.       7.      22.      10.      51.    ...   \n",
      "\n",
      "    ambient_temperature battery_id  test_id  uid   filename  \\\n",
      "0                     4      B0047        0    1  00001.csv   \n",
      "4                     4      B0047        4    5  00005.csv   \n",
      "6                     4      B0047        6    7  00007.csv   \n",
      "8                     4      B0047        8    9  00009.csv   \n",
      "10                    4      B0047       10   11  00011.csv   \n",
      "\n",
      "              Capacity   Re  Rct  \n",
      "0   1.6743047446975208  NaN  NaN  \n",
      "4   1.5243662105099023  NaN  NaN  \n",
      "6   1.5080762969973425  NaN  NaN  \n",
      "8   1.4835577960067696  NaN  NaN  \n",
      "10  1.4671391666146525  NaN  NaN  \n",
      "\n",
      "Number of discharge entries: 2794\n"
     ]
    }
   ],
   "source": [
    "discharge_metadata_df = metadata[metadata['type'] == 'discharge'].copy()\n",
    "print(\"\\nDischarge metadata head:\")\n",
    "print(discharge_metadata_df.head())\n",
    "print(f\"\\nNumber of discharge entries: {len(discharge_metadata_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "root = Path(\"/Users/siddhantaggarwal/Desktop/Battery_RUL/cleaned_dataset\")\n",
    "meta = pd.read_csv(root / \"metadata.csv\")\n",
    "\n",
    "# Coerce numeric columns\n",
    "for col in [\"Capacity\", \"Re\", \"Rct\"]:\n",
    "    meta[col] = pd.to_numeric(meta[col], errors=\"coerce\")\n",
    "\n",
    "# Filter cleanly (avoid query to sidestep dtype issues)\n",
    "dis = meta.loc[\n",
    "    (meta[\"type\"] == \"discharge\") &\n",
    "    (meta[\"Capacity\"].notna()) &\n",
    "    (meta[\"Capacity\"] > 0)\n",
    "].copy()\n",
    "\n",
    "# Cycle index per battery\n",
    "dis[\"cycle_index\"] = dis.groupby(\"battery_id\").cumcount() + 1\n",
    "\n",
    "# SOH and EOL/RUL\n",
    "cap0 = dis.groupby(\"battery_id\")[\"Capacity\"].transform(\"first\")\n",
    "dis[\"SOH\"] = dis[\"Capacity\"] / cap0\n",
    "\n",
    "eol_cycle = (dis.loc[dis[\"SOH\"] <= 0.8]\n",
    "               .groupby(\"battery_id\")[\"cycle_index\"]\n",
    "               .min()\n",
    "               .rename(\"EOL_cycle\"))\n",
    "dis = dis.merge(eol_cycle, on=\"battery_id\", how=\"left\")\n",
    "dis[\"RUL\"] = dis[\"EOL_cycle\"] - dis[\"cycle_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from 2750 files...\n",
      "  Processed 0/2750 files...\n",
      "  Processed 100/2750 files...\n",
      "  Processed 200/2750 files...\n",
      "  Processed 300/2750 files...\n",
      "  Processed 400/2750 files...\n",
      "  Processed 500/2750 files...\n",
      "  Processed 600/2750 files...\n",
      "  Processed 700/2750 files...\n",
      "  Processed 800/2750 files...\n",
      "  Processed 900/2750 files...\n",
      "  Processed 1000/2750 files...\n",
      "  Processed 1100/2750 files...\n",
      "  Processed 1200/2750 files...\n",
      "  Processed 1300/2750 files...\n",
      "  Processed 1400/2750 files...\n",
      "  Processed 1500/2750 files...\n",
      "  Processed 1600/2750 files...\n",
      "  Processed 1700/2750 files...\n",
      "  Processed 1800/2750 files...\n",
      "  Processed 1900/2750 files...\n",
      "  Processed 2000/2750 files...\n",
      "  Processed 2100/2750 files...\n",
      "  Processed 2200/2750 files...\n",
      "  Processed 2300/2750 files...\n",
      "  Processed 2400/2750 files...\n",
      "  Processed 2500/2750 files...\n",
      "  Processed 2600/2750 files...\n",
      "  Processed 2700/2750 files...\n",
      "Creating features DataFrame...\n",
      "Successfully extracted features from 2750/2750 files\n",
      "Joined dataset: 2750 rows after merge\n",
      "\n",
      "âš ï¸ Parquet save failed, saved as CSV instead: /Users/siddhantaggarwal/Desktop/Battery_RUL/data/processed/rul_features.csv\n",
      "   Error: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "   Rows: 2,750\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "root = Path(\"/Users/siddhantaggarwal/Desktop/Battery_RUL/cleaned_dataset\")\n",
    "processed_dir = Path(\"/Users/siddhantaggarwal/Desktop/Battery_RUL/data/processed\")\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Load metadata (assumes previous cell coerced numeric columns)\n",
    "meta = pd.read_csv(root / \"metadata.csv\")\n",
    "for col in [\"Capacity\", \"Re\", \"Rct\"]:\n",
    "    meta[col] = pd.to_numeric(meta[col], errors=\"coerce\")\n",
    "\n",
    "# Discharge-only rows with valid capacity\n",
    "dis = meta.loc[\n",
    "    (meta[\"type\"] == \"discharge\") & meta[\"Capacity\"].notna() & (meta[\"Capacity\"] > 0)\n",
    "].copy()\n",
    "dis[\"cycle_index\"] = dis.groupby(\"battery_id\").cumcount() + 1\n",
    "\n",
    "# SOH, EOL, RUL\n",
    "cap0 = dis.groupby(\"battery_id\")[\"Capacity\"].transform(\"first\")\n",
    "dis[\"SOH\"] = dis[\"Capacity\"] / cap0\n",
    "\n",
    "eol_cycle = (\n",
    "    dis.loc[dis[\"SOH\"] <= 0.8]\n",
    "      .groupby(\"battery_id\")[\"cycle_index\"]\n",
    "      .min()\n",
    "      .rename(\"EOL_cycle\")\n",
    ")\n",
    "dis = dis.merge(eol_cycle, on=\"battery_id\", how=\"left\")\n",
    "dis[\"RUL\"] = dis[\"EOL_cycle\"] - dis[\"cycle_index\"]\n",
    "\n",
    "# 2) Feature extraction from per-cycle waveform CSVs\n",
    "\n",
    "def extract_cycle_features(file_path: Path) -> dict:\n",
    "    try:\n",
    "        if not file_path.exists():\n",
    "            return {\"ok\": False, \"filename\": str(file_path.name)}\n",
    "        ts = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"filename\": str(file_path.name), \"error\": str(e)}\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required = {\"Voltage_measured\", \"Current_measured\", \"Temperature_measured\", \"Time\"}\n",
    "    if not required.issubset(ts.columns):\n",
    "        return {\"ok\": False, \"filename\": str(file_path.name)}\n",
    "\n",
    "    # Basic stats\n",
    "    try:\n",
    "        v = ts[\"Voltage_measured\"].astype(float)\n",
    "        i = ts[\"Current_measured\"].astype(float)\n",
    "        t = ts[\"Time\"].astype(float)\n",
    "        temp = ts[\"Temperature_measured\"].astype(float)\n",
    "    except Exception as e:\n",
    "        return {\"ok\": False, \"filename\": str(file_path.name), \"error\": str(e)}\n",
    "\n",
    "    if len(t) < 2:\n",
    "        return {\"ok\": False, \"filename\": str(file_path.name)}\n",
    "\n",
    "    # Duration (s)\n",
    "    duration_s = float(t.iloc[-1] - t.iloc[0])\n",
    "\n",
    "    # Approx coulomb count during discharge (Ah): integrate |I| over time\n",
    "    dt = np.diff(t.to_numpy(), prepend=t.iloc[0])\n",
    "    coulomb_Asec = np.sum(np.abs(i.to_numpy()) * dt)\n",
    "    coulomb_Ah = float(coulomb_Asec / 3600.0)\n",
    "\n",
    "    # Simple IR drop proxy: voltage drop between first non-zero current and the next sample\n",
    "    nz = np.where(np.abs(i.to_numpy()) > 1e-3)[0]\n",
    "    if len(nz) >= 2:\n",
    "        ir_drop = float(v.iloc[nz[0]] - v.iloc[nz[1]])\n",
    "    else:\n",
    "        ir_drop = np.nan\n",
    "\n",
    "    # Voltage/temperature features\n",
    "    feats = {\n",
    "        \"ok\": True,\n",
    "        \"duration_s\": duration_s,\n",
    "        \"voltage_mean\": float(v.mean()),\n",
    "        \"voltage_min\": float(v.min()),\n",
    "        \"voltage_max\": float(v.max()),\n",
    "        \"temp_max\": float(temp.max()),\n",
    "        \"current_mean_abs\": float(np.abs(i).mean()),\n",
    "        \"coulomb_Ah\": coulomb_Ah,\n",
    "        \"ir_drop_proxy\": ir_drop,\n",
    "    }\n",
    "    return feats\n",
    "\n",
    "# Map filenames -> features\n",
    "data_dir = root / \"data\"\n",
    "print(f\"Extracting features from {len(dis['filename'].unique())} files...\")\n",
    "records = []\n",
    "for i, fn in enumerate(dis[\"filename\"].unique()):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"  Processed {i}/{len(dis['filename'].unique())} files...\")\n",
    "    fp = data_dir / fn\n",
    "    feats = extract_cycle_features(fp)\n",
    "    feats[\"filename\"] = fn  # Always set filename\n",
    "    records.append(feats)\n",
    "\n",
    "print(\"Creating features DataFrame...\")\n",
    "feat_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# Check if we have any successful extractions\n",
    "if \"ok\" not in feat_df.columns:\n",
    "    raise ValueError(\"No 'ok' column found in feat_df. Check extract_cycle_features function.\")\n",
    "\n",
    "# Filter to successful extractions only\n",
    "successful = feat_df[\"ok\"] == True\n",
    "if successful.sum() == 0:\n",
    "    raise ValueError(\"No successful feature extractions! Check file paths and CSV structure.\")\n",
    "    \n",
    "feat_df = feat_df.loc[successful].copy()\n",
    "feat_df = feat_df.drop(columns=[\"ok\"])  # Remove the ok column\n",
    "\n",
    "print(f\"Successfully extracted features from {len(feat_df)}/{len(records)} files\")\n",
    "\n",
    "# 3) Join features with labels\n",
    "joined = dis.merge(feat_df, on=\"filename\", how=\"inner\")\n",
    "print(f\"Joined dataset: {len(joined)} rows after merge\")\n",
    "\n",
    "# 4) Create battery-level splits (train/val/test by battery_id)\n",
    "unique_batts = joined[\"battery_id\"].unique()\n",
    "if len(unique_batts) == 0:\n",
    "    raise ValueError(\"No batteries found after joining!\")\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(unique_batts)\n",
    "\n",
    "n = len(unique_batts)\n",
    "train_batts = set(unique_batts[: int(0.7 * n)])\n",
    "val_batts = set(unique_batts[int(0.7 * n) : int(0.85 * n)])\n",
    "test_batts = set(unique_batts[int(0.85 * n) :])\n",
    "\n",
    "def split_of(bid: str) -> str:\n",
    "    if bid in train_batts:\n",
    "        return \"train\"\n",
    "    if bid in val_batts:\n",
    "        return \"val\"\n",
    "    return \"test\"\n",
    "\n",
    "joined[\"split\"] = joined[\"battery_id\"].map(split_of)\n",
    "\n",
    "# 5) Save processed dataset\n",
    "out_path = processed_dir / \"rul_features.parquet\"\n",
    "try:\n",
    "    joined.to_parquet(out_path, index=False)\n",
    "    print(f\"\\nâœ… Saved: {out_path}\")\n",
    "    print(f\"   Rows: {len(joined):,}\")\n",
    "    print(f\"   Batteries: {len(unique_batts)}\")\n",
    "    print(f\"   Train: {len(joined[joined['split']=='train']):,} rows\")\n",
    "    print(f\"   Val: {len(joined[joined['split']=='val']):,} rows\")\n",
    "    print(f\"   Test: {len(joined[joined['split']=='test']):,} rows\")\n",
    "except Exception as e:\n",
    "    # Fallback to CSV if parquet fails\n",
    "    out_path_csv = processed_dir / \"rul_features.csv\"\n",
    "    joined.to_csv(out_path_csv, index=False)\n",
    "    print(f\"\\nâš ï¸ Parquet save failed, saved as CSV instead: {out_path_csv}\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"   Rows: {len(joined):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset Overview:\n",
      "   Total rows: 2,750\n",
      "   Unique batteries: 34\n",
      "   Columns: 23\n",
      "\n",
      "ðŸ“‹ Column names:\n",
      "['type', 'start_time', 'ambient_temperature', 'battery_id', 'test_id', 'uid', 'filename', 'Capacity', 'Re', 'Rct', 'cycle_index', 'SOH', 'EOL_cycle', 'RUL', 'duration_s', 'voltage_mean', 'voltage_min', 'voltage_max', 'temp_max', 'current_mean_abs', 'coulomb_Ah', 'ir_drop_proxy', 'split']\n",
      "\n",
      "ðŸ“ˆ Dataset splits:\n",
      "split\n",
      "test      589\n",
      "train    1735\n",
      "val       426\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“Š RUL Statistics:\n",
      "count    1408.000000\n",
      "mean       -3.636364\n",
      "std        42.430040\n",
      "min      -107.000000\n",
      "25%       -35.000000\n",
      "50%        -7.000000\n",
      "75%        22.000000\n",
      "max       123.000000\n",
      "Name: RUL, dtype: float64\n",
      "\n",
      "ðŸ”‹ Battery-level stats:\n",
      "           cycle_index    RUL           SOH        Capacity       \n",
      "                   max    min    max    min    max      min    max\n",
      "battery_id                                                        \n",
      "B0005              168  -67.0  100.0  0.693  1.000    1.287  1.856\n",
      "B0006              168 -107.0   60.0  0.567  1.000    1.154  2.035\n",
      "B0007              168  -44.0  123.0  0.741  1.000    1.400  1.891\n",
      "B0018              132  -57.0   74.0  0.723  1.000    1.341  1.855\n",
      "B0025               28    NaN    NaN  0.957  1.001    1.768  1.849\n",
      "B0026               28  -22.0    5.0  0.765  1.002    1.386  1.817\n",
      "B0027               28    NaN    NaN  0.971  1.000    1.770  1.823\n",
      "B0028               28    NaN    NaN  0.952  1.000    1.717  1.805\n",
      "B0029               40    NaN    NaN  0.950  1.087    1.612  1.845\n",
      "B0030               40    NaN    NaN  0.944  1.076    1.563  1.782\n",
      "\n",
      "âœ… Dataset ready for modeling!\n",
      "   Feature columns (excluding metadata): ['duration_s', 'voltage_mean', 'voltage_min', 'voltage_max', 'temp_max', 'current_mean_abs', 'coulomb_Ah', 'ir_drop_proxy']\n"
     ]
    }
   ],
   "source": [
    "# Load and explore the processed dataset\n",
    "processed_file = processed_dir / \"rul_features.csv\"\n",
    "df = pd.read_csv(processed_file)\n",
    "\n",
    "print(\"ðŸ“Š Dataset Overview:\")\n",
    "print(f\"   Total rows: {len(df):,}\")\n",
    "print(f\"   Unique batteries: {df['battery_id'].nunique()}\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "print(f\"\\nðŸ“‹ Column names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Dataset splits:\")\n",
    "print(df['split'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nðŸ“Š RUL Statistics:\")\n",
    "print(df['RUL'].describe())\n",
    "\n",
    "print(f\"\\nðŸ”‹ Battery-level stats:\")\n",
    "batt_stats = df.groupby('battery_id').agg({\n",
    "    'cycle_index': 'max',\n",
    "    'RUL': ['min', 'max'],\n",
    "    'SOH': ['min', 'max'],\n",
    "    'Capacity': ['min', 'max']\n",
    "}).round(3)\n",
    "print(batt_stats.head(10))\n",
    "\n",
    "print(f\"\\nâœ… Dataset ready for modeling!\")\n",
    "print(f\"   Feature columns (excluding metadata): {[c for c in df.columns if c not in ['battery_id', 'filename', 'type', 'start_time', 'test_id', 'uid', 'split', 'cycle_index', 'EOL_cycle', 'RUL', 'SOH', 'Capacity', 'Re', 'Rct', 'ambient_temperature']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting CSV to Parquet format...\n"
     ]
    },
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConverting CSV to Parquet format...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m df_parquet = pd.read_csv(csv_file)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mdf_parquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Saved parquet file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   CSV size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file.stat().st_size\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m(\u001b[32m1024\u001b[39m*\u001b[32m1024\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pandas/core/frame.py:3118\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3037\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3038\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3039\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3114\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pandas/io/parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pandas/io/parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pandas/io/parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pandas/core/arrays/arrow/extension_types.py:174\u001b[39m\n\u001b[32m    167\u001b[39m     pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m         ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    171\u001b[39m     pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pandas/core/arrays/arrow/extension_types.py:166\u001b[39m, in \u001b[36mpatch_pyarrow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m         pickletools.dis(serialized, out)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    159\u001b[39m             _ERROR_MSG.format(\n\u001b[32m    160\u001b[39m                 storage_type=storage_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m             )\n\u001b[32m    164\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marrow.py_extension_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m     ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m )\n\u001b[32m    171\u001b[39m pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pyarrow/types.pxi:2280\u001b[39m, in \u001b[36mpyarrow.lib.unregister_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Battery_RUL/battery_env/lib/python3.13/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "# Convert CSV to Parquet for faster loading (now that pyarrow is installed)\n",
    "csv_file = processed_dir / \"rul_features.csv\"\n",
    "parquet_file = processed_dir / \"rul_features.parquet\"\n",
    "\n",
    "if csv_file.exists():\n",
    "    print(\"Converting CSV to Parquet format...\")\n",
    "    df_parquet = pd.read_csv(csv_file)\n",
    "    df_parquet.to_parquet(parquet_file, index=False)\n",
    "    print(f\"âœ… Saved parquet file: {parquet_file}\")\n",
    "    print(f\"   CSV size: {csv_file.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"   Parquet size: {parquet_file.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"\\nðŸ’¡ Tip: Use pd.read_parquet('{parquet_file}') for faster loading in future notebooks!\")\n",
    "else:\n",
    "    print(\"CSV file not found. Skipping conversion.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing EMD extraction on: 00001.csv\n",
      "\n",
      "Cycle data shape: (490, 6)\n",
      "Columns: ['Voltage_measured', 'Current_measured', 'Temperature_measured', 'Current_load', 'Voltage_load', 'Time']\n",
      "\n",
      "âœ… Extracted 16 features\n",
      "\n",
      "Feature categories:\n",
      "\n",
      "Statistical Features (16 features):\n",
      "  Examples: duration_s, voltage_mean, voltage_min, voltage_max, voltage_std\n",
      "  ... and 11 more\n"
     ]
    }
   ],
   "source": [
    "# Test EMD Feature Extraction\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path(\"/Users/siddhantaggarwal/Desktop/Battery_RUL\").resolve()))\n",
    "\n",
    "from src.features import extract_all_features, extract_features_from_file\n",
    "import pandas as pd\n",
    "\n",
    "# Test on a single cycle file\n",
    "test_file = root / \"data\" / \"00001.csv\"\n",
    "print(f\"Testing EMD extraction on: {test_file.name}\")\n",
    "\n",
    "cycle_data = pd.read_csv(test_file)\n",
    "print(f\"\\nCycle data shape: {cycle_data.shape}\")\n",
    "print(f\"Columns: {cycle_data.columns.tolist()}\")\n",
    "\n",
    "# Extract features with EMD\n",
    "features = extract_all_features(cycle_data, include_emd=True, max_imfs=5)\n",
    "\n",
    "print(f\"\\nâœ… Extracted {len(features)} features\")\n",
    "print(f\"\\nFeature categories:\")\n",
    "feature_groups = {}\n",
    "for key in features.keys():\n",
    "    if '_imf' in key:\n",
    "        group = 'EMD Features'\n",
    "    elif key.startswith(('voltage', 'current', 'temp', 'duration', 'coulomb', 'ir')):\n",
    "        group = 'Statistical Features'\n",
    "    else:\n",
    "        group = 'Other'\n",
    "    \n",
    "    if group not in feature_groups:\n",
    "        feature_groups[group] = []\n",
    "    feature_groups[group].append(key)\n",
    "\n",
    "for group, feats in feature_groups.items():\n",
    "    print(f\"\\n{group} ({len(feats)} features):\")\n",
    "    print(f\"  Examples: {', '.join(feats[:5])}\")\n",
    "    if len(feats) > 5:\n",
    "        print(f\"  ... and {len(feats) - 5} more\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battery_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
